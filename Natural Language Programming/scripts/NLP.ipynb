{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing Nessary Libraries\n"
      ],
      "metadata": {
        "id": "WrtN3vzUd4ED"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-yp9hR1bokh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import The Dataset"
      ],
      "metadata": {
        "id": "yDvfImTufmk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "sentences = [\n",
        "    \"Running, ran, and runs are forms of the verb 'run'.\",\n",
        "    \"Visit our website at https://www.example.com or follow our blog at http://blog.example.org.\",\n",
        "    \"Important dates: 12/05/2021, 31/12/2020, 30/02/2022.\",\n",
        "    \"John's dog barks loudly, disturbing the neighbors.\",\n",
        "    \"Machine learning is a subset of artificial intelligence.\"\n",
        "]\n",
        "\n",
        "tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
        "print(tokenized_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSlmZ_gvbq7a",
        "outputId": "d46f7537-4845-484a-e9bd-e7df57001299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Running', ',', 'ran', ',', 'and', 'runs', 'are', 'forms', 'of', 'the', 'verb', \"'run\", \"'\", '.'], ['Visit', 'our', 'website', 'at', 'https', ':', '//www.example.com', 'or', 'follow', 'our', 'blog', 'at', 'http', ':', '//blog.example.org', '.'], ['Important', 'dates', ':', '12/05/2021', ',', '31/12/2020', ',', '30/02/2022', '.'], ['John', \"'s\", 'dog', 'barks', 'loudly', ',', 'disturbing', 'the', 'neighbors', '.'], ['Machine', 'learning', 'is', 'a', 'subset', 'of', 'artificial', 'intelligence', '.']]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing Stopwords from the dataset"
      ],
      "metadata": {
        "id": "WK-JRzeZf0gM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "#Remove the stopwords and set the language in english\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "#Tokenize the sentence\n",
        "sentences = []\n",
        "for tokenized_sentence in tokenized_sentences:\n",
        "    sentence = [word for word in tokenized_sentence if word.lower() not in stop_words]\n",
        "    sentences.append(sentence)\n",
        "\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXWjSUuAb46z",
        "outputId": "796cfeb9-78f1-43f1-afcd-50eb4356814e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Running', ',', 'ran', ',', 'runs', 'forms', 'verb', \"'run\", \"'\", '.'], ['Visit', 'website', 'https', ':', '//www.example.com', 'follow', 'blog', 'http', ':', '//blog.example.org', '.'], ['Important', 'dates', ':', '12/05/2021', ',', '31/12/2020', ',', '30/02/2022', '.'], ['John', \"'s\", 'dog', 'barks', 'loudly', ',', 'disturbing', 'neighbors', '.'], ['Machine', 'learning', 'subset', 'artificial', 'intelligence', '.']]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform stemming  on all words in the dataset."
      ],
      "metadata": {
        "id": "tpprK214gjB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Performing Stemming Will remove Base Root Words\n",
        "porter = PorterStemmer()\n",
        "\n",
        "stemmed_sentences = []\n",
        "for sentence in filtered_sentences:\n",
        "    stemmed_sentence = [porter.stem(word) for word in sentence]\n",
        "    stemmed_sentences.append(stemmed_sentence)\n",
        "\n",
        "print(stemmed_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mF3vCKEqcsrD",
        "outputId": "75748cf5-e0bb-459d-e01d-e4a3dc07cc87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['run', ',', 'ran', ',', 'run', 'form', 'verb', \"'run\", \"'\", '.'], ['visit', 'websit', 'http', ':', '//www.example.com', 'follow', 'blog', 'http', ':', '//blog.example.org', '.'], ['import', 'date', ':', '12/05/2021', ',', '31/12/2020', ',', '30/02/2022', '.'], ['john', \"'s\", 'dog', 'bark', 'loudli', ',', 'disturb', 'neighbor', '.'], ['machin', 'learn', 'subset', 'artifici', 'intellig', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a regex pattern to extract all URLs from the dataset."
      ],
      "metadata": {
        "id": "McFQhBrtdFjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing RegEx As re\n",
        "import re\n",
        "\n",
        "def extract_urls(text):\n",
        "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
        "    return re.findall(url_pattern, text)\n",
        "\n",
        "all_urls = []\n",
        "for sentence in sentences:\n",
        "    # Joining the list of words into a single string\n",
        "    text = ' '.join(sentence)\n",
        "    urls = extract_urls(text)\n",
        "    all_urls.extend(urls)\n",
        "\n",
        "print(all_urls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZqRVjeHc6AD",
        "outputId": "7e333d11-2e36-4999-9468-75d6acbaaa24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['www.example.com']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exporting Us Numbers"
      ],
      "metadata": {
        "id": "EWdXV3YBdMSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_us_phone_numbers(text):\n",
        "\n",
        "    phone_pattern = r'\\b(?:\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4})\\b'\n",
        "    return re.findall(phone_pattern, text)\n",
        "\n",
        "all_phone_numbers = [9340283809]\n",
        "for sentence in sentences:\n",
        "\n",
        "# Joining the words in the sentence into a single string\n",
        "    text = ' '.join(sentence)\n",
        "    phone_numbers = extract_us_phone_numbers(text)\n",
        "    all_phone_numbers.extend(phone_numbers)\n",
        "\n",
        "print(all_phone_numbers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-3VLLJkdJFD",
        "outputId": "f9da4052-e737-4892-d1f3-69d6e6ff9be5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9340283809]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kYofLgUfdQY7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}